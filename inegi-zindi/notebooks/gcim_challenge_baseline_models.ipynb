{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (images): (1100000, 16, 16, 6)\n",
      "Shape of y (labels): (1100000,)\n",
      "Shape of X_test (images): (120000, 16, 16, 6)\n"
     ]
    }
   ],
   "source": [
    "# Path to your HDF5 file\n",
    "hdf5_file = \"/teamspace/studios/this_studio/dataset/train_data.h5\"\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(hdf5_file, 'r') as hdf:\n",
    "    # Extract the images (X)\n",
    "    X = np.array(hdf['images'])\n",
    "    \n",
    "    # Extract the labels (y)\n",
    "    y = np.array(hdf['labels'])\n",
    "\n",
    "# Check the shapes to ensure they are correct\n",
    "print(\"Shape of X (images):\", X.shape)\n",
    "print(\"Shape of y (labels):\", y.shape)\n",
    "\n",
    "hdf5_file_test = \"/teamspace/studios/this_studio/dataset/test_data.h5\"\n",
    "# Open the HDF5 file\n",
    "with h5py.File(hdf5_file_test, 'r') as hdf:\n",
    "    # Extract the images (X)\n",
    "    X_test = np.array(hdf['images'])\n",
    "\n",
    "# Check the shapes to ensure they are correct\n",
    "print(\"Shape of X_test (images):\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values per channel: [-9999 -9999 -9999 -9999 -9999 -9999]\n",
      "Max values per channel: [20000 20000 20000 20000 20000 20000]\n"
     ]
    }
   ],
   "source": [
    "# Get the min and max values per channel (axis 0 is N, axis 1 and 2 are 16x16, axis 3 is the channel)\n",
    "min_per_channel = np.min(X, axis=(0, 1, 2))\n",
    "max_per_channel = np.max(X, axis=(0, 1, 2))\n",
    "\n",
    "print(\"Min values per channel:\", min_per_channel)\n",
    "print(\"Max values per channel:\", max_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 1's: 0.09090909090909091\n",
      "Proportion of 0's: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count the number of 1's in y\n",
    "num_ones = np.sum(y == 1)\n",
    "num_zeros = np.sum(y == 0)\n",
    "num_ones, num_zeros\n",
    "print(f\"Proportion of 1's: {num_ones / len(y)}\")\n",
    "print(f\"Proportion of 0's: {num_zeros / len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (images): (1100000, 16, 16, 6)\n",
      "Shape of y (labels): (1100000,)\n"
     ]
    }
   ],
   "source": [
    "# Path to your HDF5 file\n",
    "hdf5_file = \"/teamspace/studios/this_studio/dataset/optimized_train_data.h5\"\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(hdf5_file, 'r') as hdf:\n",
    "    # Extract the images (X)\n",
    "    X_opt = np.array(hdf['images'])\n",
    "    \n",
    "    # Extract the labels (y)\n",
    "    y_opt = np.array(hdf['labels'])\n",
    "\n",
    "# Check the shapes to ensure they are correct\n",
    "print(\"Shape of X (images):\", X_opt.shape)\n",
    "print(\"Shape of y (labels):\", y_opt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the y_opt is the same as y\n",
    "np.all(y_opt == y)\n",
    "\n",
    "# Checl if the X_opt is the same as X\n",
    "np.all(X_opt == X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New HDF5 file created at /teamspace/studios/this_studio/dataset/optimized_train_data.h5 with optimized chunking.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Path to your original HDF5 file\n",
    "hdf5_file = \"/teamspace/studios/this_studio/dataset/train_data.h5\"\n",
    "\n",
    "# Open the HDF5 file and extract data, dtype, and metadata\n",
    "with h5py.File(hdf5_file, 'r') as hdf:\n",
    "    # Extract the images (X) and labels (y)\n",
    "    X = np.array(hdf['images'])\n",
    "    y = np.array(hdf['labels'])\n",
    "    \n",
    "    # Extract the metadata (dtype, chunks, and attributes like band_names)\n",
    "    dtype_images = hdf['images'].dtype\n",
    "    chunk_size = hdf['images'].chunks\n",
    "    band_names = hdf['images'].attrs['band_names']\n",
    "\n",
    "# Path to save the new HDF5 file\n",
    "new_hdf5_file = \"/teamspace/studios/this_studio/dataset/optimized_train_data.h5\"\n",
    "\n",
    "# Create the new HDF5 file with optimized chunk size\n",
    "# Set chunk size to (32, 16, 16, 6) based on our earlier discussion\n",
    "new_chunk_size = (32, 16, 16, 6)\n",
    "\n",
    "with h5py.File(new_hdf5_file, 'w') as new_hdf:\n",
    "    # Create the images dataset with optimized chunk size and preserve dtype\n",
    "    images_dset = new_hdf.create_dataset('images', data=X, \n",
    "                                         chunks=new_chunk_size, \n",
    "                                         compression='gzip', \n",
    "                                         dtype=dtype_images)\n",
    "    \n",
    "    # Set the attributes, preserving the band_names\n",
    "    images_dset.attrs['band_names'] = band_names\n",
    "    \n",
    "    # Create the labels dataset with optimized chunk size\n",
    "    new_hdf.create_dataset('labels', data=y, \n",
    "                           chunks=(32,), \n",
    "                           compression='gzip', \n",
    "                           dtype=y.dtype)\n",
    "\n",
    "print(f\"New HDF5 file created at {new_hdf5_file} with optimized chunking.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create balanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (images): (1100000, 16, 16, 6)\n",
      "Shape of y (labels): (1100000,)\n",
      "Number of 1's in balanced y: 100000\n",
      "Number of 0's in balanced y: 100000\n"
     ]
    }
   ],
   "source": [
    "# Path to your HDF5 file\n",
    "hdf5_file = \"/teamspace/studios/this_studio/dataset/optimized_train_data.h5\"\n",
    "\n",
    "# Open the HDF5 file and extract data, dtype, and metadata\n",
    "with h5py.File(hdf5_file, 'r') as hdf:\n",
    "    # Extract the images (X) and labels (y)\n",
    "    X = np.array(hdf['images'])\n",
    "    y = np.array(hdf['labels'])\n",
    "    \n",
    "    # Extract the metadata (dtype, chunks, and attributes like band_names)\n",
    "    dtype_images = hdf['images'].dtype\n",
    "    chunk_size = hdf['images'].chunks\n",
    "    band_names = hdf['images'].attrs['band_names']\n",
    "\n",
    "# Check the shapes to ensure they are correct\n",
    "print(\"Shape of X (images):\", X.shape)\n",
    "print(\"Shape of y (labels):\", y.shape)\n",
    "\n",
    "# Step 1: Count the number of 1's in y\n",
    "num_ones = np.sum(y == 1)\n",
    "\n",
    "# Step 2: Get indices of 0's and 1's in y\n",
    "ones_indices = np.where(y == 1)[0]\n",
    "zeros_indices = np.where(y == 0)[0]\n",
    "\n",
    "# Step 3: Randomly sample the same number of 0's as there are 1's\n",
    "balanced_zero_indices = np.random.choice(zeros_indices, num_ones, replace=False)\n",
    "\n",
    "# Step 4: Combine indices of 0's and 1's\n",
    "balanced_indices = np.concatenate([ones_indices, balanced_zero_indices])\n",
    "\n",
    "# Step 5: Create balanced X and y\n",
    "X_balanced = X[balanced_indices]\n",
    "y_balanced = y[balanced_indices]\n",
    "\n",
    "# Display the number of 0's and 1's in the balanced y\n",
    "print(f\"Number of 1's in balanced y: {np.sum(y_balanced == 1)}\")\n",
    "print(f\"Number of 0's in balanced y: {np.sum(y_balanced == 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_balanced (images): (200000, 16, 16, 6)\n",
      "Shape of y_balanced (labels): (200000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_balanced (images):\", X_balanced.shape)\n",
    "print(\"Shape of y_balanced (labels):\", y_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New HDF5 file created at /teamspace/studios/this_studio/dataset/optimized_balanced_train_data.h5 with balanced data.\n"
     ]
    }
   ],
   "source": [
    "# Path to save the new HDF5 file\n",
    "new_hdf5_file = \"/teamspace/studios/this_studio/dataset/optimized_balanced_train_data.h5\"\n",
    "\n",
    "# Create the new HDF5 file with optimized chunk size\n",
    "# Set chunk size to (32, 16, 16, 6) based on our earlier discussion\n",
    "new_chunk_size = (32, 16, 16, 6)\n",
    "\n",
    "with h5py.File(new_hdf5_file, 'w') as new_hdf:\n",
    "    # Create the images dataset with optimized chunk size and preserve dtype\n",
    "    images_dset = new_hdf.create_dataset('images', data=X_balanced, \n",
    "                                         chunks=new_chunk_size, \n",
    "                                         compression='gzip', \n",
    "                                         dtype=dtype_images)\n",
    "    \n",
    "    # Set the attributes, preserving the band_names\n",
    "    images_dset.attrs['band_names'] = band_names\n",
    "    \n",
    "    # Create the labels dataset with optimized chunk size\n",
    "    new_hdf.create_dataset('labels', data=y_balanced, \n",
    "                           chunks=(32,), \n",
    "                           compression='gzip', \n",
    "                           dtype=y.dtype)\n",
    "\n",
    "print(f\"New HDF5 file created at {new_hdf5_file} with balanced data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
